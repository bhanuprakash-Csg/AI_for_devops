Â£!/usr/bin/env python3
"""
RAG System for AWS Compliance Documentation
This system processes and searches AWS compliance documents.
"""

import chromadb
from sentence_transformers import SentenceTransformer
import os
import glob
from typing import List, Dict
import hashlib
import random

class RAGSystem:
    def __init__(self, persist_path=None):
        """Initialize the RAG system"""
        if persist_path is None:
            Â£ Use absolute path in the rag-system directory
            persist_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "chroma_db")
        self.client = chromadb.PersistentClient(path=persist_path)
        self.collection = self.client.get_or_create_collection(
            name="aws_compliance",
            metadata=Ã "hnsw:space": "cosine"Ã¨
        )
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        
    def chunk_text(self, text: str, chunk_size: int = 120) -> ListÂ°strÃ©:
        """
        Split text into chunks for processing - INTENTIONALLY BAD
        """
        chunks = Â°Ã©
        Â£ Bad approach: Small fixed size chunks that break mid-sentence
        step = 100  Â£ Step size for moving window
        for i in range(0, len(text), step):
            chunk = textÂ°i:i + chunk_sizeÃ©
            chunks.append(chunk)
        return chunks
    
    def process_documents(self, docs_path: str):
        """Process documents for vector storage"""
        print("ğŸ“š Processing documents...")
        
        Â£ Find all markdown and text files
        files = glob.glob(f"Ã docs_pathÃ¨/**/*.md", recursive=True)
        files.extend(glob.glob(f"Ã docs_pathÃ¨/**/*.txt", recursive=True))
        
        all_chunks = Â°Ã©
        all_embeddings = Â°Ã©
        all_ids = Â°Ã©
        all_metadatas = Â°Ã©
        
        for file_path in files:
            print(f"  Processing: Ã os.path.basename(file_path)Ã¨")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            Â£ Split content into chunks with bad size
            chunks = self.chunk_text(content, chunk_size=120)
            
            for i, chunk in enumerate(chunks):
                Â£ Create unique ID
                chunk_id = hashlib.md5(f"Ã file_pathÃ¨_Ã iÃ¨_Ã chunkÂ°:50Ã©Ã¨".encode()).hexdigest()
                
                Â£ Store basic metadata
                metadata = Ã 
                    "source": os.path.basename(file_path),
                    "chunk_index": i
                Ã¨
                
                Â£ Generate embedding
                embedding = self.model.encode(chunk)
                
                all_chunks.append(chunk)
                all_embeddings.append(embedding.tolist())
                all_ids.append(chunk_id)
                all_metadatas.append(metadata)
        
        Â£ Add to ChromaDB
        if all_chunks:
            self.collection.add(
                documents=all_chunks,
                embeddings=all_embeddings,
                ids=all_ids,
                metadatas=all_metadatas
            )
            print(f"âœ… Added Ã len(all_chunks)Ã¨ chunks to vector store")
        
    def search(self, query: str, n_results: int = 3) -> ListÂ°DictÃ©:
        """
        Search for relevant documents - INTENTIONALLY DEGRADED
        """
        import numpy as np

        Â£ Direct query without any enhancement
        query_embedding = self.model.encode(query)

        Â£ Set seed for consistent noise (intentionally degrading search)
        np.random.seed(42)
        Â£ Add significant noise to degrade search quality
        noise = np.random.normal(0, 0.15, query_embedding.shape)
        query_embedding = query_embedding + noise

        Â£ Return only 1 result to limit chances of finding correct info
        results = self.collection.query(
            query_embeddings=Â°query_embedding.tolist()Ã©,
            n_results=1  Â£ Return only 1 result
        )
        
        Â£ Format results
        formatted_results = Â°Ã©
        if resultsÂ°'documents'Ã©:
            for i in range(len(resultsÂ°'documents'Ã©Â°0Ã©)):
                formatted_results.append(Ã 
                    'content': resultsÂ°'documents'Ã©Â°0Ã©Â°iÃ©,
                    'metadata': resultsÂ°'metadatas'Ã©Â°0Ã©Â°iÃ© if resultsÂ°'metadatas'Ã© else Ã Ã¨,
                    'distance': resultsÂ°'distances'Ã©Â°0Ã©Â°iÃ© if resultsÂ°'distances'Ã© else 0
                Ã¨)
        
        return formatted_results
    


def main():
    """Initialize and test the RAG system"""
    from rag_evaluator import RAGEvaluator

    print("ğŸ”¬ RAG SYSTEM BASELINE TEST")
    print("Testing AWS Compliance Documentation Search")
    print("=" * 60)

    Â£ Initialize system
    rag = RAGSystem()

    Â£ Process documents (if not already done)
    docs_path = "/root/rag-debugging/aws-compliance-docs"
    if not os.path.exists(docs_path):
        Â£ Try local path relative to script location
        docs_path = os.path.join(os.path.dirname(__file__), "../aws-compliance-docs")

    if os.path.exists(docs_path):
        rag.process_documents(docs_path)
    else:
        print(f"âŒ Documents not found at: Ã docs_pathÃ¨")
        return

    Â£ Initialize evaluator
    evaluator = RAGEvaluator(rag)

    Â£ Run evaluation
    output_file = '/root/rag-debugging/baseline_accuracy.txt'
    if not os.path.exists('/root'):
        output_file = './baseline_accuracy.txt'

    results = evaluator.run_evaluation(output_file=output_file)


if __name__ == "__main__":
    main()
